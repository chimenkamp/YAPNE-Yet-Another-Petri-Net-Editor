Check for updates
Data Petri Nets Meet Probabilistic Programming
Martin Kuhn¹, Joscha Grüger 1,2, Christoph Matheja³, and Andrey Rivkin3()
BPM ★
1 German Research Center for Artificial Intelligence (DFKI), SDS Branch Trier, Trier, Germany
martin.kuhn@dfki.de
2 University of Trier, Trier, Germany
grueger@uni-trier.de
Artifact 3 Technical University of Denmark, Kgs. Lyngby, Denmark available
{chmat,ariv}@dtu.dk

Abstract. Probabilistic programming (PP) is a programming paradigm that allows for writing statistical models like ordinary programs, performing simulations by running those programs, and analyzing and refining their statistical behavior using powerful inference engines. This paper takes a step towards leveraging PP for reasoning about data-aware processes. To this end, we present a systematic translation of Data Petri Nets (DPNs) into a model written in a PP language whose features are supported by most PP systems. We show that our translation is sound and provides statistical guarantees for simulating DPNs. Furthermore, we discuss how PP can be used for process mining tasks and report on a prototype implementation of our translation. We also discuss further analysis scenarios that could be easily approached based on the proposed translation and available PP tools.

1 Introduction
Data Petri nets (DPNs) [5,18] is a popular formalism for data-aware processes that is used in business process management (BPM) and process mining (PM) for various tasks including discovery [18], conformance checking [7,8,18], formal verification and correctness analysis [9, 10]. Moreover, it has been shown in [6] that DPNs can formalize the integration of a meaningful subset of BPMN with DMN S-FEEL decision tables. Recent work also addresses stochastic [19] and uncertainty-related [8] aspects of DPNs.

Simulation for DPNs. One of the key techniques in the BPM and PM repertoires is simulation, which allows for flexible analyses, such as "what-if" analysis, that often cannot be addressed by formal verification or that touch upon non-functional aspects (e.g., time, costs, resources) that are not reflected in process models [26,27]. For DPNs, the prime application of simulation is the generation of synthetic event logs aiming at closing the gap between missing datasets needed for substantial evaluation of discovery and analysis techniques. By far, only [13] explicitly provides a DPN simulation engine grounded in DPN semantics which allows to perform randomised generation of fixed-length executions.

The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
A. Marrella et al. (Eds.): BPM 2024, LNCS 14940, pp. 21-38, 2024.
https://doi.org/10.1007/978-3-031-70396-6_2

Probabilistic Programming (PP). [11,12] is a paradigm developed by the programming languages and machine learning communities to make statistical models and reasoning about them with Bayesian inference accessible to non-experts. The key idea is to represent statistical models as programs and leave the development of efficient simulation and inference engines to the language developers. A probabilistic program can be thought of as an ordinary program with the ability to sample from probability distributions. Running such a program means performing stochastic simulation: a single program execution corresponds to a single simulation of the underlying model. Modern PP systems have two characteristic features: First, they support conditioning the possible executions (or feasible simulations) on observed evidence, e.g. to refine a synthetic model using real-world data or user knowledge. Second, they support inference techniques to compute or approximate the probability distribution modeled by a program. This raises the question whether one can leverage the existing PP machinery for simulating DPNs instead of developing ad-hoc simulators.

Contributions and Outline. In this paper, we explore whether and how probabilistic programs are a suitable abstraction for simulating DPNs such that (1) the simulation process is based on a statistical model clearly defined as a probabilistic program and (2) simulation, event log generation, and further (statistical) analyses could benefit from using PP together with its sampling and inference capabilities (cf. [11,12]). Our main contributions can be summarized as follows:
• We formalize an execution semantics for DPNs with schedulers, which are used in discrete-event simulation to resolve non-determinism [17] (→ Sect. 3).
• We formalize the essence of many existing probabilistic programming languages (→ Sect. 4) and then develop a novel systematic encoding of DPNS with schedulers into a probabilistic programming language (→ Sect. 5).
• We show that our encoding is correct, i.e. our PP encoding of DPN produces exactly the runs of the encoded DPNs and preserves the probabilities of all simulated runs with respect to the scheduler (→ Sect. 5.3).
• We discuss how to leverage our encoding and inference engines provided by PP systems for various Process Mining tasks. (→ Sect. 6).
• We report on a proof-of-concept implementation of our encoding into the PP language webPPL [11] and discuss two case studies (→ Sect. 7).

Related Work. Multiple approaches exist that generate synthetic logs via model simulation for data-aware processes and that can be applied to DPNs. Given the known relation between colored Petri nets [14] and DPNs [5], one may use CPN Tools (https://cpntools.org/) to produce logs without noise or incompleteness [20]. As far as we know, CPN Tools is the only (non-commercial) tool in which one can explicitly define schedulers for simulation tasks. Alternatively, one may leverage connections between BPMN 2.0 models and DPNs [6] to generate multi-perspective logs. For example, in [21] the authors rely on an executable BPMN semantics that supports data objects for driving exclusive choices to generate random (multi-perspective) event longs. Similarly, [2] generates multi-perspective logs by running a randomized play-out game.

As far as we know, [13] proposes a simulation engine that directly implements the DPN execution semantics and which randomly fires enabled transitions. The random choices cannot be directly influenced, e.g. by changing the underlying scheduler. Moreover, any statistical guarantees about the generated logs provided by the enginee.g., are the generated traces representative with respect to an underlying stochastic process? are, at best, ad hoc. The same holds for the above works on the randomised log generation for BΡΜΝ. One may also rely on studied relationships between business process models and discrete event simulation (DES) [24]. For example, [22] applies DES to BPMN 2.0 models to produce multi-perspective logs. While our work is grounded in Bayesian statistics, DES takes a frequentist approach to simulation where all "variables" are already known by the domain expert. We refer to [3] for a discussion of the advantages of Bayesian approaches compared to DES.

2 Preliminaries
Sequences. A finite sequence o over a set S of length |σ|=n is a function σ:{1,...,n} -> S. We denote by e the empty sequence of length n=0. If n>0 and σ(i)=s_{i}, for 1 <= i <= n, we write σ=s_{1}...s_{n}. The set of all finite sequences over S is denoted by S*. The concatenation σ=σ_{1}σ_{2} of two sequences σ_{1},σ_{2} in S* is given by σ:{1,...,|σ_{1}|+|σ_{2}|} -> S, such that σ(i)=σ_{1}(i) for 1 <= i <= |σ_{1}|, and σ(i)=σ_{2}(i-|σ_{1}|) for |σ_{1}|+1 <= i <= |σ_{1}|+|σ_{2}|.

Probability Distributions. A (discrete) subprobability distribution over a non-empty, countable set X is a function μ:X -> [0,1] s.t. sum_{x in X} μ(x) <= 1. We say that μ(x) is the probability assigned to x in X and call a distribution if sum_{x in X} μ(x)=1. We denote by Dist(X) (resp. SubDist(X)) the set of all (sub) distributions over X, respectively. We consider a few examples:
1. The Dirac distribution δ_{x}:X -> [0,1] assigns probability 1 to a fixed element x in X and probability 0 to every other element of X.
2. For two distinct elements x,y in X and some p in [0,1], the Bernoulli distribution B(p,x,y):X -> [0,1] models a coin-flip with bias p and possible outcomes and y: it assigns p to x, (1-p) to y, and 0 otherwise.
3. The uniform distribution unif(a,b): N -> [0,1] assigns probability 1/(b-a+1) to values in [a..b]; to every other natural number r in N, it assigns 0.
4. The function B/_{2}: Q -> [0,1] given by B/_{2}(0)=B/_{2}(1)=1/4 and B/_{2}(x)=0 for all x in Q \ {0,1} is a subdistribution in SubDist(Q), which represents the Bernoulli distribution B(1/2,0,1) scaled by 1/2.
For a subdistribution μ in SubDist(X), its normalized distribution is given by normalize(μ) = μ / sum_{x in X} μ(x). For example, normalizing B/2 yields the distribution normalize(B/2) = (B/2) / (1/2) = B(1/2,0,1). As in standard probability theory, we assume 0 = 0 i.e. if assigns probability 0 everywhere, so does normalize(μ).

3 Data Petri Nets with Probabilistic Schedulers
Data Petri nets (DPNs) [5,18] extend traditional place-transition nets with the possibility to manipulate scalar case variables, which are used to constrain the evolution of the process through guards assigned to transitions. Each guard is split into a pre- and postcondition that is defined over two variable sets, V and V', where V is the set of case variables and V' keeps their primed copies used to describe variable updates. We denote by E(X, Delta) the set of all boolean expressions over variables in X and constants in A. For an expression e, V(e) and V'(e) denote the sets of all unprimed and primed variables in e, respectively.

Definition 1 (Data Petri net). A data Petri net (DPN) is a tuple N = (P, T, F, l, A, V, Delta, pre, post), where: (i) P and T are finite, disjoint sets of places and transitions, respectively; (ii) F is a subset of (P x T) U (T x P) is a flow relation; (iii) l:T -> A is a labeling function assigning activity names from A to every transition t in T; (iv) V is a set of case variables and Delta is a data domain; (v) pre: T -> E(V, Delta) is a transition precondition-assignment function; and (vi) post: T -> E(V U V', Delta) is a transition postcondition-assignment function.

Given a DPN N = (P,T, F, l, A, V, Delta, pre, post), we will from now on write P_{N}, T_{N}, etc. to denote N's components; we omit the subscript if the given net is clear from the context. Given a place or transition x in (P_{N} U T_{N}) of N, the preset x and the postset x^{bullet} are given by x={y|(y,x) in F} and x^{bullet}:={y|(x,y) in F}. We next turn to the DPN execution semantics. A state of a DPN N is a pair (M,a), where (i) M:P_{N} -> N is a total marking function, assigning a number M(p) of tokens to every place p in P_{N} and (ii) alpha:V_{N} -> Delta is a total variable valuation function assigning a value to every variable in V_{N}. A DPN moves between states by firing (enabled) transitions.

Definition 2 (Transition enabling, firing). Let N be a DPN, (M, a) a state, t in T_{N} a transition, and beta:(V_{N} U V_{N}') -> Delta_{N} be a partial valuation function. We denote by (M,alpha)[(t,beta)> that the step (t, beta) is enabled in (M,a), i.e.
(i) beta is defined for all the variables in pre(t) and post(t) and no other variables;
(ii) for every v in V_{N}(pre_{N}(t) and post_{N}(t)), we have that beta(v)=alpha(v), i.e., beta matches a on all the (non-primed) variables oft that are not being modified;
(iii) for every p in ^{bullet}t, we have M(p) >= 1;
(iv) beta models pre_{N}(t) and post_{N}(t), i.e., beta satisfies the pre- and post-conditions of t.
Moreover, we denote by (M,alpha)[(t,beta)>(M',alpha') that the state (M',alpha') is the result of firing transition t according to step (t, beta), i.e. (M,alpha)[(t,beta)> holds and (1) for every p in P_{N}, we have M'(p)=M(p)-F_{N}(p,t)+F_{N}(t,p); (2) alpha'(v)=beta(v') for all v' in V_{N}'(post(t)); and (3) alpha'(v)=alpha(v) for all v in V_{N}(pre_{N}(t) and post_{N}(t)).

A run of a DPN N is a sequence of steps sigma=(t_{1},beta_{1})...(t_{k},beta_{k}), where k in N. A state (M',alpha') is reachable from (M,a), if there is a run as above such that (M,alpha)[(t_{1},beta_{1})>(M_{1},alpha_{1})[(t_{2},beta_{2})>...[(t_{k},beta_{k})>(M',alpha'), denoted (M,alpha)[sigma>(M',alpha'). We denote by S_{N} the set of all states of N. A run o is legal iff there are two states (M, alpha), (M',alpha') in S_{N} s.t. (M,alpha)[sigma>(M',alpha').

Simulating a DPN corresponds to generating runs. A crucial part of the simulation process is how to resolve non-deterministic choices attributed to multiple, simultaneously enabled transitions and possibly infinitely many partial valuations the transitions' pre- and post-conditions. This issue has been resolved in discrete event system simulation using schedulers that handle non-deterministic choices [17]. In the same vein, we define randomized schedulers for DPNs.

Definition 3 (DPN Scheduler). A scheduler of a DPN N is a function S: S_{N} -> Dist(T_{N}) x (V_{N}' -> Dist(Delta_{N})).
In words, a scheduler S assigns to every state (M, a) a probability distribution S_{T} over the net's transitions and a mapping S_{V} from the net's (primed) variables to probability distributions over domain values. Intuitively, S_{T} resolves the nondeterminism that arises if multiple transitions are enabled: if all transitions are enabled, the probability of picking transition t is given by S_{T}(t); if some transitions are not enabled, their probability is uniformly distributed amongst the enabled transitions. The function S_{V} resolves for every v' in V_{N}' the nondeterminism that arises from postconditions that hold for different evaluations of v': the probability of assigning d in Delta_{N} to v' is given by S_{V}(v')(d). We will modify this probability by conditioning on the fact that a transition's postcondition must hold after firing it. Notice that postconditions may introduce dependencies between case variables, even though their values are sampled independently.

Example 1. Consider the DPN N in Fig. 1 representing a simple auction process, where the last offer is stored in o and the time progression is captured by t. One possible scheduler uniformly selects an enabled transition, i.e. S_{T}^{auc} = unif(1,|T_{N}|). Alternatively, one can model priorities between potentially simultaneously enabled transitions by selecting a different distribution that depends, e.g. on the value of the timer. Examples of distributions for selecting values for the variables t' and o', include the Poisson (over countable sets of timestamps and outcomes), uniform (over finite subsets of timestamps and outcomes such as S_{V}^{auc}(t')=unif(0,99) and S_{V}^{auc}(o')=unif(1,30)), geometric, and normal distribution. In the following examples, we will use S^{auc}=(S_{T}^{auc},S_{V}^{auc}) as the default scheduler.

We will now quantify the probability of a run in terms of the likelihood of each involved step. To this end, let frontier(M,alpha)={t|exists beta:(M,alpha)[(t,beta)>} be the set of transitions that are enabled in a given state. Moreover, given a Boolean proposition p, we denote by [p] its indicator function, i.e. [p]=1 if p is T (true), and 0 if p is (false).

Definition 4 (Step Likelihood). Given a net N and a scheduler S with S(M,alpha)=(S_{T},S_{V}), the likelihood of a step (t, 3) in a state (M,a) is P_{S}[M,alpha](t,beta) = P_{S}[M,alpha](t) * P_{S}[M,alpha](beta|t), where the likelihood of selecting transition t among the enabled ones is
P_{S}[M,alpha](t) = ([t in frontier(M,alpha)] * S_{T}(t)) / (sum_{t' in T_{N}}[t' in frontier(M,alpha)] * S_{T}(t'))
and the likelihood of selecting beta:(V_{N} U V_{N}') -> Delta_{N} given transition t s.t. beta(v) = alpha(v), for every v in V_{N}(pre_{N}(t) and post_{N}(t)), is
P_{S}[M,alpha](beta|t) = [beta models post_{N}(t)] * prod_{x' in V_{N}'(beta)} S_{V}(x')(beta(x')).
In the spirit of Bayes' rule, the likelihood of a step (t, 3) is the likelihood that the scheduler selects transition t multiplied with the likelihood of selecting valuation given the previous selection of t. Notice that P_{S}[M,alpha](t,beta) is 0 if the chosen transition t is not enabled and thus [t in frontier(M,alpha)]=0 or if the chosen beta does not satisfy t's postcondition and thus [beta models post_{N}(t)]=0.

Example 2. Consider the DPN and scheduler from Example 1. The likelihood of the step (timer, beta), where beta={t->20,o->5,t'->11}, in state (M, a), where M(p_{1})=M(p_{2})=1 and M(p_{0})=M(p_{3})=0 and alpha={t->20,o->5}, is P_{S^{auc}}[M,alpha](timer,beta) = (1/5) / (3/5) * 1/100.

We expand the above likelihood measure to DPN runs that reach a possibly infinite set G of goal states for the first time. Examples of goal states include all states with a final marking or all states in which x >= 15. To this end, Runs(M,alpha,G) denotes the set of all runs sigma=(t_{1},beta_{1})...(t_{n},beta_{n}) for all n>=0, such that, for all k in {1,...,n-1}, if (M,alpha)[(t_{1},beta_{1})...(t_{k},beta_{k})>(M_{k},alpha_{k}) then (M_{k},alpha_{k}) not in G. Notice that not all runs in Runs(M,alpha,G) are legal and only some may reach states from G. We then define the likelihood that a run sigma in Runs(M,alpha,G) reaches G from (M, a) via its step likelihoods, or 0 if no goal state is ever reached.

Definition 5 (Likelihood of a Run). Given a DPN N, a scheduler S, a state (M, a), goal states G, and a run sigma in Runs(M,alpha,G) over N, the likelihood P_{S}[M,alpha](sigma models G) that o reaches G from (M, a) is defined recursively as:
1, if (M,alpha) in G
P_{S}[M,alpha](t,beta) * P_{S}[M',alpha'](sigma' models G), if sigma=(t,beta)sigma' and (M,alpha)[(t,beta)>(M',alpha')
0, otherwise.
Technically, the likelihood of a run is defined analogously to the reachability probability of a trace in an (infinite-state) Markov chain a well-established stochastic model for describing sequences of events (cf. [1,23]). The probability of the next step may thus depend on the current state but is independent of previously visted states. While this assumption is common and allows for dependencies between cases, e.g. case variables can be accessed and modified by different transitions, it can be seen as a limitation because those dependencies must be modeled explicitly in the DPN's state space.

Example 3. Consider the DPN from Example 1, set of goal states G={(M,alpha) in S_{N}|M(p_{3})=1} and a run sigma=(init,{t'->10})(bid,{t->10,o'->5})(timer, {t->10,t'->0})(hammer, {t->0,o->5}). Then the likelihood to reach one of the goal states from initial state (M_{0},alpha_{0})=({p_{0}->1,p_{1},p_{2},p_{3}->0}, {t->0,o->0}) is S^{auc}[M_{0},alpha_{0}](sigma models G) = (1 * 1/100) * (1 * 1/10) * (1/2 * 1/100) * (1/3 * 1).

To obtain the probability of a run, we then normalize the above likelihood with the likelihoods of all runs that can only reach G after firing all of their steps.

Definition 6 (Probability of a Run). Given a DPN N, a scheduler S, a state (M,a), goal states G and a run o over N, the probability that reaches G given that all runs starting in (M,a) eventually reach G is defined as P_{S}[M,alpha](sigma|G) = P_{S}[M,alpha](sigma models G) / sum_{sigma' in Runs(M,alpha,G)} P_{S}[M,alpha](sigma' models G).
Notice that a run o is legal if P_{S}[M,alpha](sigma|G) > 0 holds, because the likelihood is set to 0 if we attempt to fire a transition that is not enabled or select a valuation that does not satisfy the transition's postcondition. Moreover, one can easily extract sequences of events, i.e. traces from a legal run and its initial state. Our translation to probabilistic programs in Sect. 5 will make sure that, given a scheduler S and a set of goal states G, every run o is produced with the probability P_{S}[M,alpha](sigma|G).

4 The Probabilistic Programming Language PPL
In this section, we give the necessary background on probabilistic programming by introducing a small probabilistic programming language called PPL whose features are supported by many existing PP systems (cf. [11,12]).

Definition 7 (Syntax of PPL). The set of commands C and guarded commands GC written in PPL is given by the grammar:
C ::= x := D (probabilistic assignment) | observe B (condition on event B) | log msg (add msg to the log) | C; C (sequential composition) | do GC od (unbounded loop) | if GC fi (conditional)
GC ::= B -> C (guarded command) | GC [] GC (choice)
where x is a program variable taken from a finite set Var, D is a distribution expression over Var, B is an Boolean expression over Var, msg is a message over Var and E is a (rational) probability expression over Var.

Before we formalize the details, we briefly go over the intuitive meaning of each command. The probabilistic assignment x:=D samples a value from D and assigns the result to variable x. The conditioning command observe B checks whether B holds and proceeds if the answer is yes. Otherwise, the current execution is discarded as if it never happened. The command log msg writes the value of msg to the program's log an append-only list of messages, which we will use to output runs. The sequential composition C_{1}; C_{2} first executes C_{1}, followed by C_{2}. The loop do GC od executes the guarded command GC until no guard in GC is enabled anymore. The probability of executing a command is determined by the values of the expressions E_{1},...,E_{n}. That is, if guard B_{i} holds and m is the total number of guards that currently hold, then the command C_{i} is executed with probability E_{i} * n/m. Similarly to loops, the conditional if GC fi randomly executes one of the commands in GC whose guard holds, but terminates afterwards.

Example 4. Consider a PPL program C. C first randomly assigns to x a value between 1 and 3 with probability 1/3 each. If x=1, it always assigns 4 to y. Otherwise, it either assigns 5 or x+2 to y. Since only two out of three guards hold for x>1, the probability of executing each assignment is (1/3) / (2/3) = 1/2. If, for the moment, we ignore the observe B, then the probability of terminating with y=4 is 1/3 + 1/3 * 1/2 = 1/2. Analogously, the probability to stop with y=5 is 2/3 * 1/2 + 1/3 * 1/2 = 1/2. How does conditioning on B affect those probabilities? For B=(y=5), we discard all executions except those that stop with y=5 Hence, the probability to stop with y=5 is 1, and 0 for any other value of y. For B=(x>1), we consider only those executions that assign 2 or 3 to x. Both assignments happen now with probability 1/2. Hence, y:=4 is never executed. The probability of stopping with y=4 changes to 1/2 * 1/2 = 1/4 i.e. the probability of assigning 2 to z and executing the last assignment. Analogously, the probability of stopping with y=5 changes to 3/4. For B=(x=1 and y=5), there is no feasible execution. Hence, we obtain a subdistribution that is zero for every value.

To assign formal semantics to PPL programs, we first define program states and discuss how expressions are evaluated.
States and Expressions. The set PS={s|s:Var -> Q} of program states consists of all assignments of rational numbers to program variables. Furthermore, the set of program logs PL={l|l in Msg*}, where Msg is an infinite set of messages of interest, e.g. the set of all steps of a DPN. We denote by PSL=PS x PL the set of all pairs of a state s and a program log l. We abstract from concrete syntax for expressions. Instead, we assume that, for every state, distribution expressions D evaluate to distributions over rationals, Boolean expressions B evaluate to B={T, F}, messages msg evaluate to Msg, and expressions E evaluate to rationals in [0, 1], respectively. Formally, we assume the following evaluation functions for those expressions: [B]:PS -> B, [D]:PS -> Dist(Q), [msg]:PS -> Msg, [E]:PS -> [0,1] intersect Q.

Semantics. A standard approach to assign (denotational) semantics to ordinary programs is to view them as a state transformer [C]: PS -> 2^{PS}. That is, C produces a set of output states for any given input state. The set of output states can be empty, e.g. if the program enters an infinite loop. For probabilistic programs, we obtain a more fine-grained view [15], because the probability of every output state can be quantified. Similarly, the semantics of PPL commands C is a function S[C]: PSL -> SubDist(PSL), that maps every initial state to a subdistribution over output states. We consider subdistributions, because we may lose probability mass, e.g. due to nontermination.

Definition 8 (Semantics of PPL). The (sub-)distribution computed by PPL program C for initial state-log pair (s,l) is defined as S[C](s,l) = normalize([C](s,l)) = [C](s,l) / sum_{(s',l') in PSL} [C](s,l)(s',l'), where the (unnormalized) transformer [C]: PSL -> SubDist(PSL) is defined inductively on the structure of (guarded) commands. Intuitively, S[C](s,l)(s',l') is the probability that executing PPL program C on initial program state s and log l terminates in program state s' with log l'. While our semantics is precise, we remark that there also exist "sampling-based semantics" for probabilistic programs in the literature, e.g. [4], which guarantee that, given enough samples, the computed (sub) distribution will converge towards the subdistribution [C](s,l) Depending on the chosen inference engine applied, we thus either get exact or approximate guarantees.

5 From DPNs to PPL Programs
We now develop a PPL program C_{sim} that simulates the runs of a DPN N for a given scheduler and a set of goal states such that (1) every execution of C_{sim} corresponds to a run of N and vice versa, and (2) the probability distribution of C_{sim} equals the distribution of all of the net's runs that do not visit a goal state before all of their steps have been fired. We will discuss in Sect. 6 how this distribution can be used for process mining tasks beyond simulation. We present the construction of C_{sim} step by step: we first discuss the setup and how we encode net states. In Sect. 5.2, we construct C_{sim}. Finally, Sect. 5.3 addresses why the constructed probabilistic program is correct.

5.1 Setup and Conventions
We first consider all dependencies needed for constructing C_{sim}. Throughout this section, we fix a DPN N = (P, T, F, l, A, V, Delta, pre, post), an initial state (M_{0},alpha_{0}), a scheduler S of N, and a set of goal states G. For simplicity, we assume that all data variables evaluate to rational numbers, i.e. Delta=Q that G contains all deadlocked net states, and that membership in G for non-deadlocked states can be expressed as a Boolean formula isGoal over net states. Furthermore, we assume that P={p_{1},...,p_{\#P}}, T={t_{1},...,t_{\#T}}, and V={v_{1},v_{2},...,v_{\#V}} for some natural numbers #P, #T, #V in N. We use the following program variables in our construction:
• For every place p in P, p stores how many tokens are currently in p.
• For every variable v in V, v stores the current value of v and v' is an internal program variable used for updating the value of when firing a transition.
Every underlined variable, e.g. p, corresponds to a concept of the net N, e.g. the tokens in the place p. Hence, it is straightforward to reconstruct a net state from a program state s. More formally, the marking M[s] encoded by s is given by M[s](p)=s(p) for all places p in P Analogously, the valuation alpha[s] encoded by s is given by alpha[s](v)=s(v) for all v in V_{N}.

We write S_{T} and S_{V} to refer to the transition and data component obtained from evaluating the scheduler S in the current program state. That is, if s is the current program state, then (S_{T},S_{V})=S(M[s],alpha[s]). Notice that both S_{T} and S_{V} can be represented as (distribution) expressions over program variables. Similarly, we use the Boolean expression isGoal to check whether a (non-deadlocked) state is a goal state in G. Finally, we lift our notation... to expressions over V U V'. That is, we denote by pre(t) the expression pre(t) in which every variable v in V has been replaced by v. Analogously, post(t) is the expression post(t) in which every variable v in V has been replaced by v and every variable v' in V' has been replaced by v'.

5.2 Simulating Net Runs in PPL
Intuitively, the PPL program C_{sim} simulates the runs by probabilistically selecting and firing enabled transitions in a loop until a goal state in G has been reached. In every loop iteration, will add exactly one step to the program log.

C_{init}:
p_{1}:=M_{0}(p_{1}); ...; p_{\#P}:=M_{0}(p_{\#P});
v_{1}:=alpha_{0}(v_{1}); ...; v_{\#V}:=alpha_{0}(v_{\#V});
v_{1}':=alpha_{0}(v_{1}); ...; v_{\#V}':=alpha_{0}(v_{\#V});

B_{enabled}(t): The guard checks whether transition t in T can be fired:
pre(t) and (for all p in ^{bullet}t, p >= 1).

C_{fire}(t): For a transition t in T, let ^{bullet}t={q_{1},...,q_{m}} subseteq P and t^{bullet}={r_{1},...,r_{n}} subseteq P. Moreover, let V'(post(t))={u_{1}',...,u_{k}'} be the variables that are potentially modified by firing t. Finally, we denote by step(t) the message in Msg representing a step (t, 3) of the net, where is given by the current values of the program's variables. Formally, step(t)=(t,beta), where beta(u)=u if u in V(t) and beta(u)=u' if u in V'(t). We then implement C_{fire}(t) as follows:
q_{1}:=q_{1}-1;...;q_{m}:=q_{m}-1; // remove tokens
r_{1}:=r_{1}+1;...;r_{n}:=r_{n}+1; // add tokens
u_{1}':=S_{V}(u_{1}');...;u_{k}':=S_{V}(u_{k}'); // sampling
observe post(t); // conditioning on the postcondition
log step(t); // add the just performed step to the log
u_{1}:=u_{1}';...;u_{k}:=u_{k}' // update encoded data valuation

The program first updates the program variables that encode the marking based on ^{bullet}t and t^{bullet}. We then use the scheduler component S_{V} to sample new values for all potentially modified variables. We also observe post(t) to ensure that the sampled values satisfy the postcondition. After that, we add the just performed step to the program log. Finally, we update the encoded valuation a by assigning the values of primed variables to their unprimed counterparts.

5.3 Correctness
In this section, we show how the (sub) distribution produced by the PPL program C_{sim} relates to the probabilities of runs of the encoded net N. To formalize this relationship, we call a program state s observable iff its primed and umprimed variables store the same values, i.e. s(u)=s(u') for all u in V_{N}. We denote by s_{(M,alpha)} the unique observable program state given by s_{(M,alpha)}(p)=M(p) for all p in P and s_{(M,alpha)}(u)=alpha(u) for all u in V_{N}. Our correctness theorem then intuitively states that running C_{loop} on observable states produces all legal runs of N with the same probability as N.

Theorem 1 (Correctness). Let C_{loop} be the PPL program constructed for net N, goal states G, and scheduler S in Fig. 5. For all states (M,a) of N, S[C_{loop}](s_{(M,alpha)},e) produces runs with the same probability as defined by P_{S}[M,alpha](sigma|G).

In other words, for every initial net state (M, a), executing the main loop of C_{sim} on s_{(M,alpha)} produces the same distribution over runs as the encoded net N (for the same scheduler and set of goal states). In particular, C_{init} always produces an observable program state corresponding to the initial net state (M_{0},alpha_{0}). Hence, C_{sim} produces all legal runs of N starting in (M_{0},alpha_{0}) with the same probability as the net for the given scheduler.

6 Probabilistic Programming for Process Mining Tasks
So far, we outlined how to construct a probabilistic program from a DPN with a scheduler S that, by Theorem 1, computes every DPN run with exactly the probability induced by S. Our probabilistic program can be viewed both as a program that can be executed and as a statistical model that can be further analyzed with statistical inference engines. In this section, we outline how one can leverage these views for Process Mining tasks.

Log Generation (with Guarantees). By viewing probabilistic programs as executable programs, this use case immediately follows from the run (and, eventually, trace) generation capabilities of our approach. Since every execution of the probabilistic program C_{sim} yields a DPN run, it suffices to execute C_{sim} n times to generate a data set of n DPN runs, which can be further projected to obtain an event log. Notice that such projections are done at the run-to-trace level and require matching every step (t, ) to an event (as well as well-crafted handling of silent transitions, if any). Like that, each such event carries "activity payloads" with information about all the valuations of process variables from V_{N} involved in executing non-silent activity l(t) and new valuations for the variables updated by post(t). By using PP, we get statistical guarantees on the data set (cf. [11]): the probability of the generated runs will converge to the run's probability induced by the selected scheduler. The same guarantees apply to the logs extracted from the sets of runs.

Distribution Analysis. By viewing probabilistic programs as statistical models, we can leverage statistical inference engines (cf. [11,12]) to analyze the distribution of DPN runs produced by our program. Knowing this distribution is useful to identify, for example, whether certain runs are particularly (un)likely. The true benefit, however, is that inference engines can also compute conditional probabilities, e.g. "what is the probability of reaching a marking given that the data variable is at least 17.5 and that transition t_{3} has been fired at most twice?". Technically, this can be achieved by inserting the command observe (x>17.5 and #t_{3} <= 2) in our program, where #t_{3} is an injected variable that counts the number of transition firings. After that, we run an inference engine to compute the conditional probability distribution over DPN runs in which the above observation holds. The same reasoning can be adopted for generating traces with rare events. Assume that we know from real data that a certain event rarely happens (e.g., a transition being executed twice or two data variables being equal). We can focus on such rare events using conditional probabilities and add a suitable command observe (where encodes that rare event) in the probabilistic program. A statistical inference engine will then produce the conditional probability distribution over only those runs in which the rare event (defined in) happens, which enables further analysis, e.g. what events appear frequently if holds.

What-If Analysis. Along the same vein, to test a hypothesis over the given DPN model, it suffices to modify S and/or add conditioning commands observe. As in the previous case, this will produce conditional probability distributions over those runs in which described scenario happens. Notice that such an analysis does not require any modifications of the underlying DPN and requires minor adjustments to the scheduler and/or observations, which can be easily incorporated without re-running the whole translation process discussed in Sect. 5.2.

7 Conclusion and Future Work
This paper, for the first time, establishes a systematic and proven-correct connection between data Petri nets (DPNs) and probabilistic programming, with the goal of making powerful simulation and inference engines available to DPNs. Such engines can be used for a plethora of tasks such as trace generation, complex statistical analysis of DPN runs, what-if analysis. To test the feasibility of the presented approach, we developed a proof-of-concept implementation of the translation (available at https://tinyurl.com/2xx2fhvd and https://doi.org/10.5281/zenodo.12723519) along with several examples from PNML (https://www.pnml.org) representations of DPNs into WebPPL [11] programs that can subsequently be executed in the WebPPL environment, facilitating the simulation, analysis, and inference of the statistical model they represent.

The simulator was tested on two nets: the Road Fine DPN taken from [18] (9 places, 19 transitions, 11 guards, 8 variables) and the Melanoma DPN (50 places, 76 transitions, 52 guards, 26 variables) taken from [13]. For both DPNs, we used a scheduler that uniformly selects transitions and data values. The set of goal states consists of all states that are reached by runs of some fixed length. WebPPL's MCMC inference engine was executed for various run lengths (10-50) and sample sizes (100-819200) with a 180s timeout to evaluate the performance. Figure 6 illustrates the runtime across five computation cycles. Notice that the number of generated runs, depicted on the x-axis, is doubled in every step, hence the exponential increase in runtime. Since our implementation naively follows the formal translation in Sect. 5, we consider the obtained runtimes encouraging and believe that there is ample space for optimizations.

For future work, we would like to investigate how probabilistic programming can be used to handle different types of monitoring tasks, where a partial execution prefix is given and has to be reproduced by the probabilistic program. Moreover, our current work does not explicitly support silent transitions. Finally, we want to study extended simulation setups where dependencies between different runs or non-functional criteria (e.g., case arrival time or resource allocation [24]) are also taken into account.

References
1. Baier, C., Katoen, J.: Principles of Model Checking. MIT Press, Cambridge (2008)
2. Burattin, A.: PLG2: multiperspective processes randomization and simulation for online and offline settings. arXiv, abs/1506.08415 (2015)
3. Chick, S.E.: Bayesian ideas and discrete event simulation: why, what and how. In: Perrone, L.F., Lawson, B., Liu, J., Wieland, F.P. (eds.) Proceedings of the WSC, pp. 96-105. IEEE Computer Society (2006)
4. Dahlqvist, F., Kozen, D.: Semantics of higher-order probabilistic programs with conditioning. Proc. ACM Program. Lang. 4(POPL), 57:1-57:29 (2020)
5. de Leoni, M., Felli, P., Montali, M.: A holistic approach for soundness verification of decision-aware process models. In: Trujillo, J.C., et al. (eds.) ER 2018. LNCS, vol. 11157, pp. 219-235. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-00847-5-17
6. de Leoni, M., Felli, P., Montali, M.: Integrating BPMN and DMN: modeling and analysis. J. Data Semant. 10(1-2), 165-188 (2021)
7. Felli, P., Gianola, A., Montali, M., Rivkin, A., Winkler, S.: CoCoMoT: conformance checking of multi-perspective processes via SMT. In: Polyvyanyy, A., Wynn, M.T., Van Looy, A., Reichert, M. (eds.) BPM 2021. LNCS, vol. 12875, pp. 217-234. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-85469-0-15
8. Felli, P., Gianola, A., Montali, M., Rivkin, A., Winkler, S.: Conformance checking with uncertainty via SMT. In: Di Ciccio, C., Dijkman, R., del Río Ortega, A., Rinderle-Ma, S. (eds.) BPM 2022. LNCS, vol. 13420, pp. 199-216. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-16103-2-15
9. Felli, P., Montali, M., Winkler, S.: CTL model checking for data-aware dynamic systems with arithmetic. In: Blanchette, J., Kovács, L., Pattinson, D. (eds.) IJCAR 2022. LNCS, vol. 13385, pp. 36-56. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-10769-6-4
10. Felli, P., Montali, M., Winkler, S.: Soundness of data-aware processes with arithmetic conditions. In: Franch, X., Poels, G., Gailly, F., Snoeck, M. (eds.) CAISE 2022. LNCS, vol. 13295, pp. 389-406. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-07472-1-23
11. Goodman, N.D., Stuhlmüller, A.: The Design and Implementation of Probabilistic Programming Languages (2014). http://dippl.org. Accessed 8 Mar 2024
12. Gordon, A.D., Henzinger, T.A., Nori, A.V., Rajamani, S.K.: Probabilistic programming. In: FOSE, pp. 167-181. ACM (2014)
13. Grüger, J., Geyer, T., Kuhn, M., Braun, S.A., Bergmann, R.: Verifying guideline compliance in clinical treatment using multi-perspective conformance checking: a case study. In: Munoz-Gama, J., Lu, X. (eds.) ICPM 2021. LNBIP, vol. 433, pp. 301-313. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-98581-3-22
14. Jensen, K., Kristensen, L.M.: Coloured Petri Nets Modelling and Validation of Concurrent Systems. Springer, Cham (2009)
15. Kozen, D.: Semantics of probabilistic programs. In: FOCS. IEEE (1979)
16. Kuhn, M., Grüger, J., Matheja, C., Rivkin, A.: Data Petri nets meet probabilistic programming (extended version). arXiv (2024)
17. Law, A.M.: Simulation Modeling & Analysis. McGraw-Hill, New York (2015)
18. Mannhardt, F., de Leoni, M., Reijers, H.A., van der Aalst, W.M.P.: Balanced multi-perspective checking of process conformance. Computing 98(4) (2016)
19. Mannhardt, F., Leemans, S.J.J., Schwanen, C.T., de Leoni, M.: Modelling data-aware stochastic processes discovery and conformance checking. In: Gomes, L., Lorenz, R. (eds.) PETRI NETS 2023. LNCS, vol. 13929, pp. 77-98. Springer, Cham (2023). https://doi.org/10.1007/978-3-031-33620-1-5
20. Medeiros, A., Günther, C.: Process mining: using CPN tools to create test logs for mining algorithms. In: CPN (2004)
21. Mitsyuk, A.A., Shugurov, I.S., Kalenkova, A.A., van der Aalst, W.M.: Generating event logs for high-level process models. Simul. Model. Pract. Theory 74 (2017)
22. Pufahl, L., Wong, T.Y., Weske, M.: Design of an extensible BPMN process simulator. In: Teniente, E., Weidlich, M. (eds.) BPM 2017. LNBIP, vol. 308, pp. 782-795. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-74030-0-62
23. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Programming, 1st edn. Wiley, Hoboken (1994)
24. Rosenthal, K., Ternes, B., Strecker, S.: Business process simulation on procedural graphical process models. Bus. Inf. Syst. Eng. 63(5), 569-602 (2021)
25. van de Meent, J.-W., Paige, B., Yang, H., Wood, F.: An introduction to probabilistic programming. arXiv preprint arXiv:1809.10756 (2018)
26. Aalst, W.M.P.: Business process simulation survival guide. In: vom Brocke, J., Rosemann, M. (eds.) Handbook on Business Process Management 1. IHIS, pp. 337-370. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-642-45100-3_15
27. van der Aalst, W.M.P.: Process mining and simulation: a match made in heaven! In: SummerSim, pp. 4:1-4:12. ACM (2018)